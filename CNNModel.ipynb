{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14689724,"sourceType":"datasetVersion","datasetId":9384212},{"sourceId":14785011,"sourceType":"datasetVersion","datasetId":9451760},{"sourceId":14815756,"sourceType":"datasetVersion","datasetId":9474422}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport os\n\n# --- 1. CONFIGURATION ---\nfs = 500\nwindow_sec = 4\noverlap = 0.5\nn_per_window = int(window_sec * fs)             # 2000 samples\nstep_size = int(n_per_window * (1 - overlap))   # 1000 samples step\n\n# STFT Params\nn_fft = 256\nhop_length = n_fft // 2  # 128\nwin_length = n_fft\n\n# Output Directory\noutput_base = \"dataset_stft\"\nclasses = {0: \"not_stressed\", 1: \"stressed\"}\nfor label_name in classes.values():\n    os.makedirs(os.path.join(output_base, label_name), exist_ok=True)\n\n# Labels\npre_labels = {\n    215: 1, 216: 1, 217: 0, 218: 0, 219: 0, 220: 0\n}\n\n# --- 2. GPU SETUP ---\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\nwindow_tensor = torch.hann_window(win_length).to(device)\n\ndef process_all_patients_safe_gpu_normalized():\n    total_images = 0\n    print(f\"Starting generation... Output: {output_base}/\")\n    \n    for subject_id, label in pre_labels.items():\n        filename = f\"/kaggle/input/eeg-data/subject_{subject_id}_pre.csv\"\n        \n        if not os.path.exists(filename):\n            print(f\"[WARNING] File not found: {filename}. Skipping.\")\n            continue\n            \n        label_str = classes[label]\n        print(f\"Processing {filename} -> Label: {label_str}\")\n        \n        try:\n            df = pd.read_csv(filename)\n            eeg_data = df.iloc[:, 1:32]\n            \n            for col_name in eeg_data.columns:\n                signal = eeg_data[col_name].values\n                signal = signal - np.mean(signal)\n                \n                window_count = 0\n                for start_idx in range(0, len(signal) - n_per_window + 1, step_size):\n                    end_idx = start_idx + n_per_window\n                    chunk = signal[start_idx:end_idx]\n                    \n                    # --- GPU CALCULATION ---\n                    chunk_tensor = torch.tensor(chunk, dtype=torch.float32).to(device)\n                    \n                    stft_out = torch.stft(\n                        chunk_tensor, \n                        n_fft=n_fft, \n                        hop_length=hop_length, \n                        win_length=win_length,\n                        window=window_tensor,\n                        center=True,\n                        return_complex=True\n                    )\n                    \n                    magnitude = torch.abs(stft_out)\n                    \n                    # --- THE FIX: NORMALIZE TO MATCH SCIPY ---\n                    # We divide by (Window Length / 2) to normalize the amplitude\n                    magnitude = magnitude / (win_length / 2)\n                    \n                    magnitude_db = 20 * torch.log10(magnitude + 1e-10)\n                    img_data = magnitude_db.cpu().numpy()\n                    \n                    # --- PLOTTING ---\n                    fig = plt.figure(figsize=(4, 4))\n                    ax = plt.Axes(fig, [0., 0., 1., 1.])\n                    ax.set_axis_off()\n                    fig.add_axes(ax)\n                    \n                    ax.imshow(img_data, aspect='auto', origin='lower', cmap='inferno', vmin=-20, vmax=40)\n                    \n                    save_name = f\"{subject_id}_pre_{col_name}_{window_count:04d}.png\"\n                    save_path = os.path.join(output_base, label_str, save_name)\n                    \n                    plt.savefig(save_path)\n                    plt.close(fig)\n                    \n                    window_count += 1\n                    total_images += 1\n            \n        except Exception as e:\n            print(f\"[ERROR] Failed on {filename}: {e}\")\n\n    print(f\"\\nProcessing Complete! Total Images Generated: {total_images}\")\n\nif __name__ == \"__main__\":\n    process_all_patients_safe_gpu_normalized()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T15:55:34.344775Z","iopub.execute_input":"2026-02-10T15:55:34.345226Z","iopub.status.idle":"2026-02-10T15:55:37.253050Z","shell.execute_reply.started":"2026-02-10T15:55:34.345201Z","shell.execute_reply":"2026-02-10T15:55:37.251985Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nStarting generation... Output: dataset_stft/\nProcessing /kaggle/input/eeg-data/subject_215_pre.csv -> Label: stressed\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/829812098.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m     \u001b[0mprocess_all_patients_safe_gpu_normalized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_55/829812098.py\u001b[0m in \u001b[0;36mprocess_all_patients_safe_gpu_normalized\u001b[0;34m()\u001b[0m\n\u001b[1;32m     94\u001b[0m                     \u001b[0msave_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_base\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m                     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m                     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36msavefig\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1241\u001b[0m     \u001b[0;31m# savefig default implementation has no return, so mypy is unhappy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m     \u001b[0;31m# presumably this is here because subclasses can return?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1243\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[func-returns-value]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1244\u001b[0m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_idle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Need this if 'transparent=True', to reset colors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1245\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36msavefig\u001b[0;34m(self, fname, transparent, **kwargs)\u001b[0m\n\u001b[1;32m   3488\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0max\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3489\u001b[0m                     \u001b[0m_recursively_make_axes_transparent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3490\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3492\u001b[0m     def ginput(self, n=1, timeout=30, show_clicks=True,\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2182\u001b[0m                 \u001b[0;31m# force the figure dpi to 72), so we need to set it again here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2183\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2184\u001b[0;31m                     result = print_method(\n\u001b[0m\u001b[1;32m   2185\u001b[0m                         \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2186\u001b[0m                         \u001b[0mfacecolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfacecolor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   2038\u001b[0m                 \"bbox_inches_restore\"}\n\u001b[1;32m   2039\u001b[0m             \u001b[0mskip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptional_kws\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2040\u001b[0;31m             print_method = functools.wraps(meth)(lambda *args, **kwargs: meth(\n\u001b[0m\u001b[1;32m   2041\u001b[0m                 *args, **{k: v for k, v in kwargs.items() if k not in skip}))\n\u001b[1;32m   2042\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Let third-parties do as they see fit.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mprint_png\u001b[0;34m(self, filename_or_obj, metadata, pil_kwargs)\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0;34m*\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincluding\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0;34m'Software'\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m         \"\"\"\n\u001b[0;32m--> 481\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_print_pil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"png\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpil_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprint_to_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36m_print_pil\u001b[0;34m(self, filename_or_obj, fmt, pil_kwargs, metadata)\u001b[0m\n\u001b[1;32m    427\u001b[0m         *pil_kwargs* and *metadata* are forwarded).\n\u001b[1;32m    428\u001b[0m         \"\"\"\n\u001b[0;32m--> 429\u001b[0;31m         \u001b[0mFigureCanvasAgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m         mpl.image.imsave(\n\u001b[1;32m    431\u001b[0m             \u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer_rgba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfmt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morigin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"upper\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    380\u001b[0m         with (self.toolbar._wait_cursor_for_draw_cm() if self.toolbar\n\u001b[1;32m    381\u001b[0m               else nullcontext()):\n\u001b[0;32m--> 382\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m             \u001b[0;31m# A GUI class may be need to update a window using this draw, so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[0;31m# don't forget to call the superclass.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdraw_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rasterizing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_rasterizing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   3255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3256\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3257\u001b[0;31m                 mimage._draw_list_compositing_images(\n\u001b[0m\u001b[1;32m   3258\u001b[0m                     renderer, self, artists, self.suppressComposite)\n\u001b[1;32m   3259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   3179\u001b[0m             \u001b[0m_draw_rasterized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0martists_rasterized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3181\u001b[0;31m         mimage._draw_list_compositing_images(\n\u001b[0m\u001b[1;32m   3182\u001b[0m             renderer, self, artists, self.get_figure(root=True).suppressComposite)\n\u001b[1;32m   3183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m    597\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m             im, l, b, trans = self.make_image(\n\u001b[0m\u001b[1;32m    600\u001b[0m                 renderer, renderer.get_image_magnification())\n\u001b[1;32m    601\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mmake_image\u001b[0;34m(self, renderer, magnification, unsampled)\u001b[0m\n\u001b[1;32m    900\u001b[0m         clip = ((self.get_clip_box() or self.axes.bbox) if self.get_clip_on()\n\u001b[1;32m    901\u001b[0m                 else self.get_figure(root=True).bbox)\n\u001b[0;32m--> 902\u001b[0;31m         return self._make_image(self._A, bbox, transformed_bbox, clip,\n\u001b[0m\u001b[1;32m    903\u001b[0m                                 magnification, unsampled=unsampled)\n\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_make_image\u001b[0;34m(self, A, in_bbox, out_bbox, clip_bbox, magnification, unsampled, round_to_pixel_border)\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m                 \u001b[0;31m# resample the input data to the correct resolution and shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 474\u001b[0;31m                 \u001b[0mA_resampled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_resample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaled_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m                 \u001b[0;31m# if using NoNorm, cast back to the original datatype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_resample\u001b[0;34m(image_obj, data, out_shape, transform, resample, alpha)\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresample\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0mresample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_resample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m     _image.resample(data, out, transform,\n\u001b[0m\u001b[1;32m    211\u001b[0m                     \u001b[0m_interpd_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m                     \u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"},{"output_type":"display_data","data":{"text/plain":"<Figure size 400x400 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"labels = { \n201: {\"pre\": 1, \"post\": 0},\n202: {\"pre\": 0, \"post\": 0},\n203: {\"pre\": 1, \"post\": 0},\n204: {\"pre\": 0, \"post\": 0},\n205: {\"pre\": 1, \"post\": 0},\n206: {\"pre\": 0, \"post\": 0},\n207: {\"pre\": 1, \"post\": 1},\n208: {\"pre\": 0, \"post\": 0},\n209: {\"pre\": 1, \"post\": 0},\n211: {\"pre\": 1, \"post\": 0},\n212: {\"pre\": 1, \"post\": 0},\n213: {\"pre\": 0, \"post\": 0},\n214: {\"pre\": 0, \"post\": 0},\n215: {\"pre\": 1, \"post\": 0},\n216: {\"pre\": 1, \"post\": 0},\n217: {\"pre\": 0, \"post\": 0},\n218: {\"pre\": 0, \"post\": 0},\n219: {\"pre\": 0, \"post\": 0},\n220: {\"pre\": 0, \"post\": 0},\n}","metadata":{}},{"cell_type":"code","source":"import os\nimport glob\n\n# Defines\noutput_base = \"/kaggle/working/dataset_stft\"\nclasses = [\"stressed\", \"not_stressed\"]\nsubject_ids = [\n    201, 202, 203, 204, 205, 206, 207, 208, 209, \n    211, 212, 213, 214, 215, 216, 217, 218, 219, 220\n]\n\nprint(\"Checking image counts per patient...\")\nprint(f\"{'Subject ID':<12} | {'Count':<10} | {'Status'}\")\nprint(\"-\" * 35)\n\nfor sub_id in subject_ids:\n    # Search for files starting with this ID in both folders\n    pattern = f\"{sub_id}_pre_*.png\"\n    \n    count = 0\n    for label in classes:\n        folder_path = os.path.join(output_base, label)\n        # Use glob to count files matching the pattern\n        files = glob.glob(os.path.join(folder_path, pattern))\n        count += len(files)\n        \n    status = \"‚úÖ OK\" if count > 2500 else \"‚ùå MISSING\"\n    print(f\"{sub_id:<12} | {count:<10} | {status}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T12:36:06.059456Z","iopub.execute_input":"2026-02-09T12:36:06.060145Z","iopub.status.idle":"2026-02-09T12:36:06.769427Z","shell.execute_reply.started":"2026-02-09T12:36:06.060114Z","shell.execute_reply":"2026-02-09T12:36:06.768843Z"}},"outputs":[{"name":"stdout","text":"Checking image counts per patient...\nSubject ID   | Count      | Status\n-----------------------------------\n201          | 2790       | ‚úÖ OK\n202          | 2945       | ‚úÖ OK\n203          | 2759       | ‚úÖ OK\n204          | 2945       | ‚úÖ OK\n205          | 1860       | ‚ùå MISSING\n206          | 1860       | ‚ùå MISSING\n207          | 1829       | ‚ùå MISSING\n208          | 1860       | ‚ùå MISSING\n209          | 1860       | ‚ùå MISSING\n211          | 1860       | ‚ùå MISSING\n212          | 1829       | ‚ùå MISSING\n213          | 1829       | ‚ùå MISSING\n214          | 1860       | ‚ùå MISSING\n215          | 1860       | ‚ùå MISSING\n216          | 1829       | ‚ùå MISSING\n217          | 1829       | ‚ùå MISSING\n218          | 1829       | ‚ùå MISSING\n219          | 1829       | ‚ùå MISSING\n220          | 1829       | ‚ùå MISSING\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import shutil\nimport os\nfrom IPython.display import FileLink\n\n# --- 1. CONFIGURATION ---\nfolder_to_zip = \"dataset_stft\"  # The folder you created\noutput_filename = \"dataset_stft\" # The name of the zip file\n\n# --- 2. ZIP THE FOLDER ---\nprint(f\"Zipping '{folder_to_zip}'... This may take a few minutes (50k+ files).\")\n\n# This creates 'dataset_stft_v3.zip' in your working directory\nshutil.make_archive(output_filename, 'zip', folder_to_zip)\n\nprint(f\"Success! Created {output_filename}.zip\")\n\n# --- 3. GENERATE DOWNLOAD LINK ---\n# Click the link below to download immediately\nFileLink(f'{output_filename}.zip')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T12:40:14.177028Z","iopub.execute_input":"2026-02-09T12:40:14.177750Z","iopub.status.idle":"2026-02-09T12:40:36.149631Z","shell.execute_reply.started":"2026-02-09T12:40:14.177708Z","shell.execute_reply":"2026-02-09T12:40:36.149047Z"}},"outputs":[{"name":"stdout","text":"Zipping 'dataset_stft'... This may take a few minutes (50k+ files).\nSuccess! Created dataset_stft.zip\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/dataset_stft.zip","text/html":"<a href='dataset_stft.zip' target='_blank'>dataset_stft.zip</a><br>"},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"CNN - model","metadata":{}},{"cell_type":"code","source":"import os\nimport shutil\nimport random\n\n# Source Directory (Where your 35k images are)\nsource_dir = \"/kaggle/input/dataset-stft-filter\"\ntrain_dir = \"dataset_final/train\"\nval_dir = \"dataset_final/val\"\n\n# --- 1. DEFINE THE SPLIT MANUALLY ---\n# We pick 4 validation subjects (2 Stressed, 2 Not Stressed) to ensure balance.\nval_subjects = [215, 216, 218, 220]  \n# 215, 216 = Stressed (1)\n# 218, 220 = Not Stressed (0)\n\n# The rest go to training\nall_subjects = [\n    201, 202, 203, 204, 205, 206, 207, 208, 209, \n    211, 212, 213, 214, 215, 216, 217, 218, 219, 220\n]\ntrain_subjects = [s for s in all_subjects if s not in val_subjects]\n\nprint(f\"Training Subjects: {len(train_subjects)}\")\nprint(f\"Validation Subjects: {len(val_subjects)}\")\n\n# --- 2. MOVE THE FILES ---\nclasses = [\"stressed\", \"not_stressed\"]\n\nfor label in classes:\n    # Create dirs\n    os.makedirs(os.path.join(train_dir, label), exist_ok=True)\n    os.makedirs(os.path.join(val_dir, label), exist_ok=True)\n    \n    # List all images in the source folder\n    src_folder = os.path.join(source_dir, label)\n    images = os.listdir(src_folder)\n    \n    for img in images:\n        # Check the filename to see which patient it belongs to\n        # Filename format: \"201_pre_Fp1_0000.png\"\n        subject_id = int(img.split('_')[0])\n        \n        src_path = os.path.join(src_folder, img)\n        \n        if subject_id in val_subjects:\n            dst_path = os.path.join(val_dir, label, img)\n        else:\n            dst_path = os.path.join(train_dir, label, img)\n            \n        shutil.copy(src_path, dst_path)\n\nprint(\"Dataset Split Complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T08:16:41.779025Z","iopub.execute_input":"2026-02-12T08:16:41.779661Z","iopub.status.idle":"2026-02-12T08:20:04.522473Z","shell.execute_reply.started":"2026-02-12T08:16:41.779628Z","shell.execute_reply":"2026-02-12T08:20:04.521662Z"}},"outputs":[{"name":"stdout","text":"Training Subjects: 15\nValidation Subjects: 4\nDataset Split Complete!\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nimport os\n\n# --- 1. CONFIGURATION ---\nBATCH_SIZE = 32\n# Start with a slightly higher LR, let the Scheduler lower it later\nLEARNING_RATE = 0.0001  \nEPOCHS = 40             \nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nTRAIN_DIR = \"/kaggle/working/dataset_final/train\"\nVAL_DIR = \"/kaggle/working/dataset_final/val\"\n\nprint(f\"üöÄ Golden Training on: {DEVICE}\")\n\n# --- 2. DATA (Same Heavy Augmentation) ---\ntrain_transforms = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.RandomAffine(degrees=0, translate=(0.1, 0.05)),\n    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\nval_transforms = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\ntrain_dataset = datasets.ImageFolder(TRAIN_DIR, transform=train_transforms)\nval_dataset = datasets.ImageFolder(VAL_DIR, transform=val_transforms)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\n# --- 3. 5-LAYER MODEL (Same as before) ---\nclass BrainStressCNN5Layer(nn.Module):\n    def __init__(self):\n        super(BrainStressCNN5Layer, self).__init__()\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n        self.conv5 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n        \n        self.pool = nn.MaxPool2d(2, 2)\n        self.relu = nn.ReLU()\n        self.flatten = nn.Flatten()\n        \n        self.fc1 = nn.Linear(256 * 7 * 7, 512)\n        self.dropout = nn.Dropout(0.5) \n        self.fc2 = nn.Linear(512, 2)\n\n    def forward(self, x):\n        x = self.pool(self.relu(self.conv1(x)))\n        x = self.pool(self.relu(self.conv2(x)))\n        x = self.pool(self.relu(self.conv3(x)))\n        x = self.pool(self.relu(self.conv4(x)))\n        x = self.pool(self.relu(self.conv5(x)))\n        x = self.flatten(x)\n        x = self.dropout(self.relu(self.fc1(x)))\n        x = self.fc2(x)\n        return x\n\nmodel = BrainStressCNN5Layer().to(DEVICE)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n\n# --- NEW: SCHEDULER ---\n# If Val Acc doesn't improve for 3 epochs, cut LR by 10x\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=3)\n\n# --- 4. TRAINING LOOP WITH CHECKPOINTING ---\nprint(\"\\nüî• Starting Training with Checkpointing...\")\nbest_val_acc = 0.0\n\nfor epoch in range(EPOCHS):\n    model.train()\n    correct, total = 0, 0\n    \n    for inputs, labels in train_loader:\n        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n    \n    train_acc = 100 * correct / total\n    \n    # Validation\n    model.eval()\n    val_correct, val_total = 0, 0\n    with torch.no_grad():\n        for inputs, labels in val_loader:\n            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs.data, 1)\n            val_total += labels.size(0)\n            val_correct += (predicted == labels).sum().item()\n            \n    val_acc = 100 * val_correct / val_total\n    \n    # Update Scheduler\n    scheduler.step(val_acc)\n    \n    # MANUAL PRINT: Check current Learning Rate\n    current_lr = optimizer.param_groups[0]['lr']\n    print(f\"   -> Current Learning Rate: {current_lr}\")\n    \n    # --- CRITICAL: SAVE ONLY IF IMPROVED ---\n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        torch.save(model.state_dict(), \"best_stress_model.pth\")\n        print(f\"Epoch {epoch+1} | Train: {train_acc:.2f}% | Val: {val_acc:.2f}% --> üíæ Saved Best Model!\")\n    else:\n        print(f\"Epoch {epoch+1} | Train: {train_acc:.2f}% | Val: {val_acc:.2f}%\")\n\nprint(f\"\\nüèÜ Done! Best Validation Accuracy was: {best_val_acc:.2f}%\")\nprint(\"Load 'best_stress_model.pth' for your final predictions.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T08:27:04.427279Z","iopub.execute_input":"2026-02-12T08:27:04.428000Z","iopub.status.idle":"2026-02-12T08:52:53.756522Z","shell.execute_reply.started":"2026-02-12T08:27:04.427969Z","shell.execute_reply":"2026-02-12T08:52:53.755231Z"}},"outputs":[{"name":"stdout","text":"üöÄ Golden Training on: cuda\n\nüî• Starting Training with Checkpointing...\n   -> Current Learning Rate: 0.0001\nEpoch 1 | Train: 60.62% | Val: 43.81% --> üíæ Saved Best Model!\n   -> Current Learning Rate: 0.0001\nEpoch 2 | Train: 68.07% | Val: 51.30% --> üíæ Saved Best Model!\n   -> Current Learning Rate: 0.0001\nEpoch 3 | Train: 69.98% | Val: 47.60%\n   -> Current Learning Rate: 0.0001\nEpoch 4 | Train: 70.65% | Val: 43.60%\n   -> Current Learning Rate: 0.0001\nEpoch 5 | Train: 71.65% | Val: 43.92%\n   -> Current Learning Rate: 1e-05\nEpoch 6 | Train: 72.71% | Val: 44.93%\n   -> Current Learning Rate: 1e-05\nEpoch 7 | Train: 73.98% | Val: 43.21%\n   -> Current Learning Rate: 1e-05\nEpoch 8 | Train: 74.18% | Val: 44.29%\n   -> Current Learning Rate: 1e-05\nEpoch 9 | Train: 74.62% | Val: 43.19%\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/3001824259.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcorrect\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":8},{"cell_type":"markdown","source":"üöÄ 5-Layer Robust Training on: cuda\n\nüî• Starting Training...\n\nEpoch 1/30 | Train Acc: 67.69% | Val Acc: 46.51%\n\nEpoch 2/30 | Train Acc: 75.60% | Val Acc: 49.00%\n\nEpoch 3/30 | Train Acc: 78.60% | Val Acc: 60.83%\n\nEpoch 4/30 | Train Acc: 80.22% | Val Acc: 53.82%\n\nEpoch 5/30 | Train Acc: 81.22% | Val Acc: 60.20%\n\nEpoch 6/30 | Train Acc: 82.38% | Val Acc: 51.42%\n\nEpoch 7/30 | Train Acc: 83.49% | Val Acc: 50.99%\n\nEpoch 8/30 | Train Acc: 84.38% | Val Acc: 50.32%\n\nEpoch 9/30 | Train Acc: 85.39% | Val Acc: 65.37%","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms, models\nimport os\n\n# --- 1. CONFIGURATION ---\nBATCH_SIZE = 32\nLEARNING_RATE = 0.0001  # Transformers need a much lower LR!\nEPOCHS = 20\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nTRAIN_DIR = \"/kaggle/working/dataset_final/train\"\nVAL_DIR = \"/kaggle/working/dataset_final/val\"\n\nprint(f\"üöÄ Training Vision Transformer (ViT) on: {DEVICE}\")\n\n# --- 2. DATA LOADERS ---\n# ViT expects 224x224.\n# We keep augmentation light because ViTs are sensitive.\ntrain_transforms = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\nval_transforms = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\ntrain_dataset = datasets.ImageFolder(TRAIN_DIR, transform=train_transforms)\nval_dataset = datasets.ImageFolder(VAL_DIR, transform=val_transforms)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\n# --- 3. LOAD PRE-TRAINED VIT ---\nprint(\"üß† Loading ViT-Base-16 (ImageNet Weights)...\")\n\n# Load the model with default weights\nweights = models.ViT_B_16_Weights.DEFAULT\nmodel = models.vit_b_16(weights=weights)\n\n# --- 4. MODIFY THE HEAD ---\n# ViT has a 'heads' block. The classifier is inside.\n# We freeze the backbone first to stabilize training (Optional, but recommended for ViT)\nfor param in model.parameters():\n    param.requires_grad = False \n\n# Unfreeze the last block and the head so they can learn\nfor param in model.encoder.layers.encoder_layer_11.parameters():\n    param.requires_grad = True\n\n# Replace the head\nnum_features = model.heads.head.in_features\nmodel.heads.head = nn.Linear(num_features, 2) # 2 Classes\n\nmodel = model.to(DEVICE)\n\n# --- 5. OPTIMIZER ---\ncriterion = nn.CrossEntropyLoss()\n# ViT is very sensitive. Use a small LR and Weight Decay.\noptimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=2)\n\n# --- 6. TRAINING LOOP ---\nprint(\"\\nüî• Starting ViT Training...\")\nbest_val_acc = 0.0\n\nfor epoch in range(EPOCHS):\n    model.train()\n    correct, total = 0, 0\n    \n    for inputs, labels in train_loader:\n        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n    \n    train_acc = 100 * correct / total\n    \n    # Validation\n    model.eval()\n    val_correct, val_total = 0, 0\n    with torch.no_grad():\n        for inputs, labels in val_loader:\n            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs.data, 1)\n            val_total += labels.size(0)\n            val_correct += (predicted == labels).sum().item()\n            \n    val_acc = 100 * val_correct / val_total\n    \n    scheduler.step(val_acc)\n    \n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        torch.save(model.state_dict(), \"best_stress_vit.pth\")\n        print(f\"Epoch {epoch+1} | Train: {train_acc:.2f}% | Val: {val_acc:.2f}% --> üíæ Saved Best!\")\n    else:\n        print(f\"Epoch {epoch+1} | Train: {train_acc:.2f}% | Val: {val_acc:.2f}%\")\n\nprint(f\"\\nüèÜ Best ViT Accuracy: {best_val_acc:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T05:38:19.790840Z","iopub.execute_input":"2026-02-12T05:38:19.791547Z","iopub.status.idle":"2026-02-12T08:14:54.567829Z","shell.execute_reply.started":"2026-02-12T05:38:19.791516Z","shell.execute_reply":"2026-02-12T08:14:54.566980Z"}},"outputs":[{"name":"stdout","text":"üöÄ Training Vision Transformer (ViT) on: cuda\nüß† Loading ViT-Base-16 (ImageNet Weights)...\nDownloading: \"https://download.pytorch.org/models/vit_b_16-c867db91.pth\" to /root/.cache/torch/hub/checkpoints/vit_b_16-c867db91.pth\n","output_type":"stream"},{"name":"stderr","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 330M/330M [00:01<00:00, 177MB/s]  \n","output_type":"stream"},{"name":"stdout","text":"\nüî• Starting ViT Training...\nEpoch 1 | Train: 71.47% | Val: 42.43% --> üíæ Saved Best!\nEpoch 2 | Train: 79.12% | Val: 46.94% --> üíæ Saved Best!\nEpoch 3 | Train: 81.80% | Val: 49.07% --> üíæ Saved Best!\nEpoch 4 | Train: 84.10% | Val: 51.46% --> üíæ Saved Best!\nEpoch 5 | Train: 86.28% | Val: 50.99%\nEpoch 6 | Train: 87.54% | Val: 48.37%\nEpoch 7 | Train: 89.12% | Val: 49.52%\nEpoch 8 | Train: 92.24% | Val: 50.93%\nEpoch 9 | Train: 92.87% | Val: 49.35%\nEpoch 10 | Train: 93.11% | Val: 48.37%\nEpoch 11 | Train: 93.62% | Val: 49.12%\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/384345345.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0mcorrect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    732\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m             if (\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    788\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 790\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    791\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    243\u001b[0m         \"\"\"\n\u001b[1;32m    244\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mdefault_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maccimage_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpil_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    984\u001b[0m             \u001b[0mdeprecate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0mhas_transparency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"transparency\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m                             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m                             \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":4},{"cell_type":"markdown","source":".","metadata":{}},{"cell_type":"markdown","source":".","metadata":{}},{"cell_type":"markdown","source":".\n","metadata":{}},{"cell_type":"markdown","source":".","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms, models\nimport os\nimport copy\n\n# --- 1. CONFIGURATION ---\nBATCH_SIZE = 32\nLEARNING_RATE = 0.0005  # EfficientNet prefers lower LR\nEPOCHS = 30\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nTRAIN_DIR = \"/kaggle/working/dataset_final/train\" # Your new filtered data\nVAL_DIR = \"/kaggle/working/dataset_final/val\"\n\nprint(f\"üöÄ Training EfficientNet-B0 on: {DEVICE}\")\n\n# --- 2. DATA LOADERS ---\n# EfficientNet expects 224x224 RGB images\n# ImageFolder automatically converts your Grayscale to RGB (3 channels)\ntrain_transforms = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.RandomAffine(degrees=0, translate=(0.1, 0.05)),\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\nval_transforms = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\ntrain_dataset = datasets.ImageFolder(TRAIN_DIR, transform=train_transforms)\nval_dataset = datasets.ImageFolder(VAL_DIR, transform=val_transforms)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T09:00:32.438319Z","iopub.execute_input":"2026-02-12T09:00:32.438920Z","iopub.status.idle":"2026-02-12T09:00:32.543718Z","shell.execute_reply.started":"2026-02-12T09:00:32.438886Z","shell.execute_reply":"2026-02-12T09:00:32.543003Z"}},"outputs":[{"name":"stdout","text":"üöÄ Training EfficientNet-B0 on: cuda\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# --- 3. MODEL: EFFICIENTNET-B0 ---\nprint(\"üß† Loading Pre-trained EfficientNet...\")\n# Download weights trained on ImageNet (14M images)\nweights = models.EfficientNet_B0_Weights.DEFAULT\nmodel = models.efficientnet_b0(weights=weights)\n\n# Freeze the early layers (Optional: keeps the basic feature detectors stable)\n# for param in model.features.parameters():\n#     param.requires_grad = False\n\n# Modify the Classifier Head\n# EfficientNet's classifier is a Sequential block. We replace the last Linear layer.\nnum_features = model.classifier[1].in_features\nmodel.classifier[1] = nn.Sequential(\n    nn.Dropout(p=0.3),\n    nn.Linear(num_features, 2) # 2 Classes: Stressed vs Not\n)\n\nmodel = model.to(DEVICE)\n\n# --- 4. OPTIMIZER ---\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.2, patience=3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T09:00:56.679426Z","iopub.execute_input":"2026-02-12T09:00:56.679744Z","iopub.status.idle":"2026-02-12T09:00:56.826840Z","shell.execute_reply.started":"2026-02-12T09:00:56.679717Z","shell.execute_reply":"2026-02-12T09:00:56.826098Z"}},"outputs":[{"name":"stdout","text":"üß† Loading Pre-trained EfficientNet...\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# --- 5. TRAINING LOOP ---\nprint(\"\\nüî• Starting EfficientNet Training...\")\nbest_val_acc = 0.0\nbest_model_wts = copy.deepcopy(model.state_dict())\n\nfor epoch in range(EPOCHS):\n    model.train()\n    correct = 0\n    total = 0\n    running_loss = 0.0\n    \n    for inputs, labels in train_loader:\n        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n        \n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n    \n    train_acc = 100 * correct / total\n    epoch_loss = running_loss / len(train_loader)\n    \n    # Validation\n    model.eval()\n    val_correct = 0\n    val_total = 0\n    \n    with torch.no_grad():\n        for inputs, labels in val_loader:\n            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs.data, 1)\n            val_total += labels.size(0)\n            val_correct += (predicted == labels).sum().item()\n            \n    val_acc = 100 * val_correct / val_total\n    \n    print(f\"Epoch {epoch+1}/{EPOCHS} | Train Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f}% | LR: {optimizer.param_groups[0]['lr']:.6f}\")\n    \n    scheduler.step(val_acc)\n    \n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        best_model_wts = copy.deepcopy(model.state_dict())\n        torch.save(model.state_dict(), \"best_efficientnet_stress.pth\")\n        print(\"   --> üíæ Saved Best Model!\")\n\nprint(f\"\\nüèÜ Best EfficientNet Accuracy: {best_val_acc:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T09:01:09.875671Z","iopub.execute_input":"2026-02-12T09:01:09.875967Z","iopub.status.idle":"2026-02-12T09:29:24.289488Z","shell.execute_reply.started":"2026-02-12T09:01:09.875942Z","shell.execute_reply":"2026-02-12T09:29:24.288103Z"}},"outputs":[{"name":"stdout","text":"\nüî• Starting EfficientNet Training...\nEpoch 1/30 | Train Acc: 67.45% | Val Acc: 43.32% | LR: 0.000500\n   --> üíæ Saved Best Model!\nEpoch 2/30 | Train Acc: 73.81% | Val Acc: 46.60% | LR: 0.000500\n   --> üíæ Saved Best Model!\nEpoch 3/30 | Train Acc: 75.88% | Val Acc: 44.41% | LR: 0.000500\nEpoch 4/30 | Train Acc: 77.31% | Val Acc: 47.38% | LR: 0.000500\n   --> üíæ Saved Best Model!\nEpoch 5/30 | Train Acc: 77.85% | Val Acc: 40.22% | LR: 0.000500\nEpoch 6/30 | Train Acc: 79.23% | Val Acc: 39.80% | LR: 0.000500\nEpoch 7/30 | Train Acc: 79.77% | Val Acc: 40.82% | LR: 0.000500\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/2614268840.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    732\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m             if (\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    788\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 790\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    791\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    243\u001b[0m         \"\"\"\n\u001b[1;32m    244\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mdefault_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maccimage_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpil_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    984\u001b[0m             \u001b[0mdeprecate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0mhas_transparency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"transparency\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m                             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m                             \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":14},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nimport os\n\n# --- 1. CONFIGURATION ---\nBATCH_SIZE = 32\nLEARNING_RATE = 0.0003  # LSTMs need gentle learning rates\nEPOCHS = 40\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nTRAIN_DIR = \"/kaggle/working/dataset_final/train\"\nVAL_DIR = \"/kaggle/working/dataset_final/val\"\n\nprint(f\"üöÄ Training CRNN (Hybrid) on: {DEVICE}\")\n\n# --- 2. DATA LOADERS ---\n# CRNNs are sensitive to rotation/flipping because Time flows one way!\n# So we REMOVE RandomHorizontalFlip.\ntrain_transforms = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.RandomAffine(degrees=0, translate=(0.1, 0.05)), # Shift is okay\n    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\nval_transforms = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\ntrain_dataset = datasets.ImageFolder(TRAIN_DIR, transform=train_transforms)\nval_dataset = datasets.ImageFolder(VAL_DIR, transform=val_transforms)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T09:29:31.168769Z","iopub.execute_input":"2026-02-12T09:29:31.169468Z","iopub.status.idle":"2026-02-12T09:29:31.271070Z","shell.execute_reply.started":"2026-02-12T09:29:31.169428Z","shell.execute_reply":"2026-02-12T09:29:31.270447Z"}},"outputs":[{"name":"stdout","text":"üöÄ Training CRNN (Hybrid) on: cuda\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"\n# --- 3. THE CRNN MODEL ---\nclass EEG_CRNN(nn.Module):\n    def __init__(self):\n        super(EEG_CRNN, self).__init__()\n        \n        # --- PART 1: CNN (Feature Extractor) ---\n        # We want to crush the Height (Freq) but keep some Width (Time)\n        \n        # Input: (3, 224, 224)\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2) # -> (64, 112, 112)\n        )\n        \n        self.conv2 = nn.Sequential(\n            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2) # -> (128, 56, 56)\n        )\n        \n        self.conv3 = nn.Sequential(\n            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.MaxPool2d((2, 1)) # Pool Height only! Keep Time resolution.\n            # -> (256, 28, 56)\n        )\n        \n        self.conv4 = nn.Sequential(\n            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(),\n            nn.MaxPool2d((2, 1)) # Pool Height only!\n            # -> (512, 14, 56)\n        )\n        \n        # After Conv4, we have:\n        # Channels: 512\n        # Height (Freq features): 14\n        # Width (Time steps): 56\n        \n        # We collapse Height into Channels\n        self.cnn_output_size = 512 * 14 \n        \n        # --- PART 2: LSTM (Sequence Modeler) ---\n        self.lstm = nn.LSTM(\n            input_size=self.cnn_output_size, \n            hidden_size=256, \n            num_layers=2, \n            batch_first=True,\n            dropout=0.5,\n            bidirectional=True # Look forward and backward in time\n        )\n        \n        # --- PART 3: CLASSIFIER ---\n        # Bidirectional = 2 * hidden_size\n        self.fc = nn.Sequential(\n            nn.Linear(256 * 2, 128),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(128, 2)\n        )\n\n    def forward(self, x):\n        # 1. Run CNN\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        x = self.conv4(x)\n        \n        # Shape: (Batch, 512, 14, 56)\n        b, c, h, w = x.size()\n        \n        # 2. Reshape for LSTM\n        # We want: (Batch, Time, Features)\n        # Permute to: (Batch, Width, Channels, Height) -> (Batch, 56, 512, 14)\n        x = x.permute(0, 3, 1, 2) \n        \n        # Flatten Channels and Height: (Batch, 56, 512*14)\n        x = x.reshape(b, w, -1)\n        \n        # 3. Run LSTM\n        # out: (Batch, Time, Hidden*2)\n        # _ : (hidden state, cell state) - we ignore these\n        lstm_out, _ = self.lstm(x)\n        \n        # 4. Classification\n        # We take the output of the LAST time step\n        # Shape: (Batch, Hidden*2)\n        last_time_step = lstm_out[:, -1, :]\n        \n        output = self.fc(last_time_step)\n        return output\n\nmodel = EEG_CRNN().to(DEVICE)\n\n# --- 4. TRAINING SETUP ---\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3)\n\n# --- 5. TRAINING LOOP ---\nprint(\"\\nüî• Starting CRNN Training...\")\nbest_val_acc = 0.0\n\nfor epoch in range(EPOCHS):\n    model.train()\n    correct = 0\n    total = 0\n    \n    for inputs, labels in train_loader:\n        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n        \n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n    \n    train_acc = 100 * correct / total\n    \n    # Validation\n    model.eval()\n    val_correct = 0\n    val_total = 0\n    \n    with torch.no_grad():\n        for inputs, labels in val_loader:\n            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs.data, 1)\n            val_total += labels.size(0)\n            val_correct += (predicted == labels).sum().item()\n            \n    val_acc = 100 * val_correct / val_total\n    \n    print(f\"Epoch {epoch+1}/{EPOCHS} | Train Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f}%\")\n    \n    scheduler.step(val_acc)\n    \n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        torch.save(model.state_dict(), \"best_stress_crnn.pth\")\n        print(\"   --> üíæ Saved Best Model!\")\n\nprint(f\"\\nüèÜ Best CRNN Accuracy: {best_val_acc:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T09:29:35.609686Z","iopub.execute_input":"2026-02-12T09:29:35.610003Z","iopub.status.idle":"2026-02-12T10:15:48.306439Z","shell.execute_reply.started":"2026-02-12T09:29:35.609975Z","shell.execute_reply":"2026-02-12T10:15:48.305386Z"}},"outputs":[{"name":"stdout","text":"\nüî• Starting CRNN Training...\nEpoch 1/40 | Train Acc: 55.82% | Val Acc: 41.16%\n   --> üíæ Saved Best Model!\nEpoch 2/40 | Train Acc: 60.49% | Val Acc: 48.58%\n   --> üíæ Saved Best Model!\nEpoch 3/40 | Train Acc: 62.74% | Val Acc: 45.87%\nEpoch 4/40 | Train Acc: 64.66% | Val Acc: 50.50%\n   --> üíæ Saved Best Model!\nEpoch 5/40 | Train Acc: 66.12% | Val Acc: 48.14%\nEpoch 6/40 | Train Acc: 68.16% | Val Acc: 37.65%\nEpoch 7/40 | Train Acc: 68.80% | Val Acc: 47.08%\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/24580494.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcorrect\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":16},{"cell_type":"code","source":"print(1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T13:59:23.848693Z","iopub.execute_input":"2026-02-12T13:59:23.848963Z","iopub.status.idle":"2026-02-12T13:59:23.858097Z","shell.execute_reply.started":"2026-02-12T13:59:23.848932Z","shell.execute_reply":"2026-02-12T13:59:23.857243Z"}},"outputs":[{"name":"stdout","text":"1\n","output_type":"stream"}],"execution_count":1}]}